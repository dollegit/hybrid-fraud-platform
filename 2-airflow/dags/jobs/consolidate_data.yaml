apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: consolidate-data-job
  namespace: spark-jobs
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "psalmprax/spark-app:1.0.0"
  mainApplicationFile: "local:///opt/spark/work-dir/consolidate_data.py"
  imagePullPolicy: IfNotPresent
  sparkVersion: "3.5.7"
  restartPolicy:
    type: OnFailure

  # This job needs JDBC drivers to write to PostgreSQL
  sparkConf:
    "spark.driver.extraClassPath": "/opt/spark/jars/*"
    "spark.executor.extraClassPath": "/opt/spark/jars/*"
    "spark.jars": >
      local:///opt/spark/jars/postgresql-42.6.0.jar

  driver:
    cores: 1
    memory: "1g"
    serviceAccount: "spark"
    env:
      # The script needs the JDBC URL and credentials to connect to the staging DB (PostgreSQL)
      - name: STAGING_DB_URL
        value: "jdbc:postgresql://airflow-postgresql.airflow.svc.cluster.local:5432/airflow"
      - name: STAGING_DB_USER
        value: "airflow"
      - name: STAGING_DB_PASSWORD
        value: "airflow"
      # The script needs to know where the input data is located
      - name: DATA_INPUT_PATH
        value: "/opt/spark/data"
    volumeMounts:
      # Mount the PVC to read the generated data
      - name: spark-data-volume
        mountPath: /opt/spark/data

  executor:
    cores: 1
    instances: 1
    memory: "1g"
    env:
      # Pass the data path to executors as well, in case they need it
      - name: DATA_INPUT_PATH
        value: "/opt/spark/data"
    volumeMounts:
      # Mount the PVC on executors so they can read the data
      - name: spark-data-volume
        mountPath: /opt/spark/data

  # Define the volume using the same PVC as the generation job
  volumes:
    - name: spark-data-volume
      persistentVolumeClaim:
        claimName: spark-pvc